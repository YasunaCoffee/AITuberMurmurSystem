### **プロジェクト名：Monologue Agent v2 要件定義書**

#### 1. 開発背景と目的

**1.1. 開発背景**
現行の「Monologue Agent v1」は、AI VTuberとしての基本的な機能（コメントへの応答、独り言による会話継続など）を実現している。しかし、そのアーキテクチャは、複数のバックグラウンドスレッドが `is_speaking` や `is_thinking` といった状態フラグを直接的かつ非同期的に更新する設計となっている。
この設計は、今回のデバッグで明らかになったように、以下の深刻な問題点を内包している。

*   **競合状態（Race Condition）:** 複数のスレッドが意図しないタイミングで状態を上書きし、システムのデッドロックや予期せぬ停止を引き起こす。
*   **デバッグの困難さ:** 問題が発生した際に、どのスレッドが、どのタイミングで、どの状態を変更したのかを追跡することが非常に困難。
*   **保守性の低下:** 新しい機能を追加したり、既存のロジックを修正したりする際に、他の部分に意図しない副作用（リグレッション）を及ぼすリスクが高い。

**1.2. 開発目的**
Monologue Agent v2は、これらのアーキテクチャ上の問題を根本的に解決し、以下の目標を達成することを目的とする。

*   **安定性の飛躍的向上:** 競合状態が発生しない設計により、長時間安定して稼働するシステムを構築する。
*   **保守性と拡張性の確保:** 各コンポーネントの役割を明確に分離し、将来的な機能追加や仕様変更が容易な、見通しの良いコードベースを実現する。
*   **テスト容易性の確立:** システムの各部分が独立して、かつ連携した状態でテスト可能な設計を採用し、品質を継続的に保証できる体制を築く。

---

#### 2. V2のコアコンセプト

V2の成功は、以下の3つのコアコンセプトを徹底することにかかっている。

**2.1. イベント駆動型アーキテクチャ (Event-Driven Architecture)**
「状態フラグを監視して判断する」という現在のモデルを完全に撤廃し、「発生したイベント（出来事）に応じて処理を実行する」モデルに移行する。

*   **イベントキュー:** システムの中心に単一の「イベント/コマンドキュー」を配置する。すべての処理のきっかけは、このキューに投入されるイベントとなる。
*   **イベントの例:** `SpeechPlaybackCompleted`（発話完了）、`NewCommentReceived`（新規コメント受信）、`MonologueReady`（独り言生成完了）など。
*   **コマンドの例:** `PrepareMonologue`（独り言を準備せよ）、`PlaySpeech`（音声再生せよ）、`RespondToComments`（コメントに応答せよ）など。

**2.2. 責務の明確な分離 (Clear Separation of Concerns)**
各コンポーネント（クラス）は、単一の明確な役割のみを持つ。

*   **マネージャー/アダプター層 (例: `AudioManager`):** 外部とのやり取り（音声再生、コメント取得など）と、それに伴う**イベントの発行**のみを担当する。「次に何をすべきか」は判断しない。
*   **ハンドラー層 (例: `MonologueHandler`):** 特定の**コマンドの実行**のみを担当する。OpenAIへの問い合わせなど、具体的なビジネスロ-ジックはここに集約する。
*   **コントローラー層 (例: `MainController`):** システムの唯一の「頭脳」。イベントキューからイベントを受け取り、現在の状態に応じて「次に何をすべきか（どのコマンドを発行すべきか）」を**判断**することのみを担当する。

**2.3. 状態の一元管理 (Centralized State Management)**
システムの現在の状態（例：会話履歴、現在のモードなど）は、`StateManager`に集約する。状態を変更する権限は、原則として`MainController`のみが持つ。これにより、予期せぬ副作用を防ぎ、状態の変化を追跡しやすくする。

---

#### 3. 主要コンポーネントと責務

| コンポーネント | 責務（やること） | やらないこと |
| :--- | :--- | :--- |
| **イベント/コマンドキュー** | イベントとコマンドをFIFO（先入れ先出し）で保持する。 | 処理の判断、実行 |
| **`MainController`** | ①キューからイベントを取得<br>②`StateManager`の状態を参照<br>③次に実行すべきコマンドを判断<br>④キューにコマンドを投入 | OpenAIへの問い合わせ、音声再生、コメント取得などの実処理 |
| **`MonologueHandler`** | `PrepareMonologue`コマンドを受け取り、OpenAIに独り言を問い合わせ、`MonologueReady`イベントをキューに投入する。 | 実行タイミングの自己判断、状態の直接変更 |
| **`CommentHandler`** | `RespondToComments`コマンドを受け取り、OpenAIに応答を問い合わせ、`CommentResponseReady`イベントをキューに投入する。 | 実行タイミングの自己判断、状態の直接変更 |
| **`AudioManager`** | `PlaySpeech`コマンドを受け取り、音声を再生する。再生が完了したら`SpeechPlaybackCompleted`イベントをキューに投入する。 | 次の独り言の準備リクエスト、状態の直接変更 |
| **`IntegratedCommentManager`**| 新しいコメントを取得したら`NewCommentReceived`イベントをキューに投入する。 | AIの応答生成、音声再生 |
| **`StateManager`** | システム全体の静的な状態（会話履歴、モードなど）を保持するデータコンテナ。 | 処理ロジック、イベントの発行 |

---

#### 4. 機能要件

v1の機能を、v2のアーキテクチャに沿って再定義する。

**4.1. 独り言の自動生成**
1.  `AudioManager`が発話完了後、`SpeechPlaybackCompleted`イベントをキューに投入する。
2.  `MainController`がイベントを取得。コメントキューが空であることを確認し、`PrepareMonologue`コマンドをキューに投入する。
3.  `MonologueHandler`がコマンドを取得。OpenAIに独り言を問い合わせ、`MonologueReady`イベント（文章リストを含む）をキューに投入する。
4.  `MainController`がイベントを取得。`PlaySpeech`コマンド（文章リストを含む）をキューに投入する。
5.  `AudioManager`がコマンドを取得し、再生を開始する。

**4.2. コメントへの応答**
1.  `IntegratedCommentManager`が新規コメントを取得後、`NewCommentReceived`イベント（コメント内容を含む）をキューに投入する。
2.  `MainController`がイベントを取得。現在の状態（発話中でないなど）を判断し、`RespondToComments`コマンドをキューに投入する。
3.  以降のフローは「4.1. 独り言の自動生成」の3〜5と同様。

**4.3. その他機能**
*   **初期挨拶:** 起動時に`MainController`が`PrepareMonologue`コマンド（初期挨拶用プロンプトを指定）を自己発行する。
*   **終了挨拶:** 終了シグナルをイベントとして扱い、`MainController`が終了挨拶用のコマンドを発行する。

---

#### 5. 非機能要件

**5.1. 安定性**
*   特定のコンポーネント（例：OpenAI API）でエラーが発生した場合でも、システム全体が停止しないこと。エラーはイベントとして処理され、`MainController`がフォールバック処理（例：「ちょっと考えがまとまりません」と発話するコマンドを発行）を実行する。
*   長時間（最低8時間以上）の連続稼働が可能であること。

**5.2. 保守性・拡張性**
*   各コンポーネントの責務が単一であるため、修正や機能追加が他のコンポーネントに影響を与えにくい構造であること。
*   Python標準の`logging`モジュールによる構造化ロギングを導入し、デバッグや動作分析を容易にすること。

**5.3. テスト容易性**
*   **ユニットテスト:** 各ハンドラーは、特定のコマンドを受け取って期待されるイベントを発行するかを、独立してテスト可能であること。
*   **統合テスト:** `MainController`に特定のイベントシーケンスを入力し、期待されるコマンドシーケンスが出力されるかをテスト可能であること。

---

#### 6. 移行計画案（推奨）

大規模な一括書き換えはリスクが高いため、以下の段階的な移行を推奨する。

1.  **Phase 1: 基盤構築:** イベントキューと、新しい責務を持つ`MainController`の雛形を実装する。
2.  **Phase 2: コンポーネントの移行:** まずは`AudioManager`から、イベントを発行しコマンドを受け取る形にリファクタリングする。次に`MonologueHandler`、`CommentHandler`と、一つずつ新しいアーキテクチャに適応させていく。
3.  **Phase 3: テストの拡充:** 移行が完了したコンポーネントから順に、新しいアーキテクチャに基づいたユニットテストと統合テストを記述していく。

---

以上が、Monologue Agent v2の要件定義書となります。
この内容で、より安定し、将来性のあるシステムの開発に進んでいければと考えておりますが、いかがでしょうか。ご意見や追加のご要望があれば、ぜひお聞かせください。

---

#### 7. Appendix: V1からの資産流用計画

V2への移行は、ゼロからの再開発ではなく、V1の資産を最大限に活用するリファクタリングです。以下に、V1の各コンポーネントがV2のアーキテクチャにおいて、どのように位置づけられ、再利用されるかの計画を示します。

**7.1. ほぼそのまま流用可能（変更が軽微なもの）**

これらのファイルは、特定のビジネスロジックや外部サービスとの接続など、V2のアーキテクチャの根幹とは独立した機能を提供しているため、ほぼそのまま、あるいは僅かな修正で再利用できます。

| ファイル/フォルダ | V1での役割・機能 | V2での位置づけ |
| :--- | :--- | :--- |
| `prompts/` | LLMに与える指示（プロンプト）群 | 全てそのまま流用。ハンドラークラスが利用します。 |
| `config.py`, `config.yaml` | APIキーや設定値 | 全てそのまま流用。各コンポーネントが利用します。 |
| `openai_adapter.py` | OpenAI APIとの通信 | コアロジックは流用。ハンドラークラスが利用します。 |
| `aivis_speech_adapter.py` | AIViSpeech APIとの通信 | コアロジックは流用。`AudioManager`が利用します。 |
| `memory_manager.py` | 長期記憶（ベクトルDB）の管理 | コアロジックは流用。主にハンドラーが記憶検索に利用します。 |
| `conversation_history.py` | 会話履歴の保存・読込 | コアロジックは流用。`StateManager`が保持し、`MainController`が更新を指示します。|
| `advanced_text_processor.py`| テキストの前処理・後処理 | 文章を整形するユーティリティとしてそのまま流用可能です。 |
| `txt/` | NGワードなどのデータファイル | そのまま流用可能です。 |

**7.2. コアロジックは流用可能だが、大幅なリファクタリングが必要なもの**

これらのファイルはV1の中核を担っていましたが、V2では責務が大きく変わります。内部の具体的な処理（音声再生、プロンプト生成など）は流用しつつ、全体を新しいインターフェース（コマンド受信、イベント発行）に合わせて書き換える必要があります。

| ファイル/フォルダ | V1での役割 | V2での新しい役割 |
| :--- | :--- | :--- |
| `audio_manager.py` | 音声合成・再生と状態管理（`is_speaking`） | `PlaySpeech`コマンドを受け取って音声を再生し、完了後に`SpeechPlaybackCompleted`イベントを発行する責務に特化します。 |
| `handlers/*.py` | 応答生成と`AudioManager`への直接指示 | 対応する`Prepare...`コマンドを受け取り、LLMに応答を生成し、`...Ready`イベントを発行する責務に特化します。 |
| `integrated_comment_manager.py`| コメントの取得とキューへの提供 | 定期的にコメントを取得し、新しいものがあれば`NewCommentReceived`イベントを発行する責務に特化します。 |
| `mode_manager.py` | 会話モードの管理 | `MainController`やハンドラーが、プロンプト選択などの判断材料として利用するデータ提供クラスになります。 |
| `theme_manager.py` | 会話テーマの管理 | `mode_manager`と同様、データ提供クラスになります。 |

**7.3. ほぼ書き直しが必要なもの**

これらのファイルは、V1の「状態フラグ監視」アーキテクチャそのものであるため、V2の「イベント駆動」アーキテクチャでは根本的に役割が変わり、コードのほとんどが新しいロジックに置き換わります。

| ファイル/フォルダ | V1での役割 | V2での新しい役割 |
| :--- | :--- | :--- |
| `controllers/main_controller.py` | 状態フラグを監視して各処理を呼び出すメインループ | イベントキューからイベントを取得し、状態に応じて次に実行すべきコマンドを判断し、キューに投入する「頭脳」になります。 |
| `controllers/state_manager.py` | `threading.Event`などによる動的な状態管理 | `threading.Event`を撤廃し、会話履歴や現在のモードなど、より静的な状態を保持する純粋なデータコンテナになります。 |

**7.4. 参考資料として有用なもの**

| ファイル/フォルダ | V1での役割・機能 | V2での位置づけ |
| :--- | :--- | :--- |
| `tests/` | V1のユニットテスト群 | V1のアーキテクチャに依存するため直接は使えませんが、「V1でどの機能がテストされていたか」を知るための重要な参考資料となります。V2のテストを設計する際のインプットとして非常に価値があります。 |

**結論として、V1の多くのコンポーネントが持つ「具体的な処理を行う能力」（例: OpenAIに聞く、音声を再生する）はV2でも非常に価値のある資産です。** V2開発の主な作業は、これらの能力を「イベントとコマンド」という新しい共通言語で再接続し、`MainController`という唯一の司令塔の下で整然と動作するようにリファクタリングすることになります。 